{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:47:23.483236Z",
     "iopub.status.busy": "2025-10-15T15:47:23.482696Z",
     "iopub.status.idle": "2025-10-15T15:47:30.566481Z",
     "shell.execute_reply": "2025-10-15T15:47:30.565608Z",
     "shell.execute_reply.started": "2025-10-15T15:47:23.483209Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.6.15)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!pip install -q pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:47:33.906985Z",
     "iopub.status.busy": "2025-10-15T15:47:33.906530Z",
     "iopub.status.idle": "2025-10-15T15:47:33.911735Z",
     "shell.execute_reply": "2025-10-15T15:47:33.910971Z",
     "shell.execute_reply.started": "2025-10-15T15:47:33.906958Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Device name: Tesla P100-PCIE-16GB\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"GPU is NOT available, using CPU instead.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:47:33.913428Z",
     "iopub.status.busy": "2025-10-15T15:47:33.913168Z",
     "iopub.status.idle": "2025-10-15T15:47:34.843536Z",
     "shell.execute_reply": "2025-10-15T15:47:34.842970Z",
     "shell.execute_reply.started": "2025-10-15T15:47:33.913410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import ast\n",
    "from pyvi import ViTokenizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:00:26.954819Z",
     "iopub.status.busy": "2025-10-13T16:00:26.954174Z",
     "iopub.status.idle": "2025-10-13T16:11:18.904812Z",
     "shell.execute_reply": "2025-10-13T16:11:18.904036Z",
     "shell.execute_reply.started": "2025-10-13T16:00:26.954796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/soict-hackathon-2024-legal-document-retrieval/Train/train.csv\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokenizing question...\")\n",
    "df[\"question\"] = df[\"question\"].progress_apply(tokenize_text)\n",
    "\n",
    "print(\"Tokenizing context...\")\n",
    "def process_context(x):\n",
    "    try:\n",
    "        lst = ast.literal_eval(x)\n",
    "        if isinstance(lst, list):\n",
    "            return [ViTokenizer.tokenize(i) for i in lst]\n",
    "        else:\n",
    "            return [ViTokenizer.tokenize(str(lst))]\n",
    "    except Exception:\n",
    "        return [ViTokenizer.tokenize(str(x))]\n",
    "\n",
    "df[\"context\"] = df[\"context\"].progress_apply(process_context)\n",
    "\n",
    "print(\"Processing cid...\")\n",
    "def process_cid(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except Exception:\n",
    "        return [int(x)] if str(x).isdigit() else []\n",
    "df[\"cid\"] = df[\"cid\"].progress_apply(process_cid)\n",
    "\n",
    "df.to_csv(\"train_tokenized.csv\", index=False)\n",
    "\n",
    "print(\"Finished train_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:11:18.906223Z",
     "iopub.status.busy": "2025-10-13T16:11:18.905920Z",
     "iopub.status.idle": "2025-10-13T16:11:28.416350Z",
     "shell.execute_reply": "2025-10-13T16:11:28.415495Z",
     "shell.execute_reply.started": "2025-10-13T16:11:18.906200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/working/train_tokenized.csv\")\n",
    "\n",
    "def clean_cid(x):\n",
    "    try:\n",
    "        val = ast.literal_eval(x)\n",
    "        if isinstance(val, list) and len(val) > 0:\n",
    "            return val[0]\n",
    "        else:\n",
    "            return np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "print(\"Cleaning cid column...\")\n",
    "df[\"cid\"] = df[\"cid\"].progress_apply(clean_cid)\n",
    "\n",
    "df.to_csv(\"/kaggle/working/train_tokenized.csv\", index=False)\n",
    "\n",
    "print(\"Done! File saved as train_tokenized.csv\")\n",
    "print(df[[\"question\",\"cid\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:11:28.417633Z",
     "iopub.status.busy": "2025-10-13T16:11:28.417350Z",
     "iopub.status.idle": "2025-10-13T16:11:36.334348Z",
     "shell.execute_reply": "2025-10-13T16:11:36.333490Z",
     "shell.execute_reply.started": "2025-10-13T16:11:28.417612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/working/train_tokenized.csv\")\n",
    "df = df.dropna(subset=[\"cid\"])\n",
    "df[\"cid\"] = df[\"cid\"].astype(int)\n",
    "df.to_csv(\"/kaggle/working/train_tokenized.csv\", index=False)\n",
    "print(\"Done! File saved as train_tokenized.csv\")\n",
    "print(df[[\"question\",\"cid\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:11:36.336132Z",
     "iopub.status.busy": "2025-10-13T16:11:36.335880Z",
     "iopub.status.idle": "2025-10-13T16:11:39.234303Z",
     "shell.execute_reply": "2025-10-13T16:11:39.233389Z",
     "shell.execute_reply.started": "2025-10-13T16:11:36.336114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/working/train_tokenized.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train - val - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:47:39.430292Z",
     "iopub.status.busy": "2025-10-15T15:47:39.429816Z",
     "iopub.status.idle": "2025-10-15T15:47:48.206963Z",
     "shell.execute_reply": "2025-10-15T15:47:48.205904Z",
     "shell.execute_reply.started": "2025-10-15T15:47:39.430267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 86575\n",
      "Validation: 9620\n",
      "Test: 10689\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/datause/train_tokenized.csv\")\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "train_df.to_csv(\"train_split.csv\", index=False)\n",
    "val_df.to_csv(\"val_split.csv\", index=False)\n",
    "test_df.to_csv(\"test_split.csv\", index=False)\n",
    "\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Validation: {len(val_df)}\")\n",
    "print(f\"Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Segmentation for corpus set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:11:49.283177Z",
     "iopub.status.busy": "2025-10-13T16:11:49.282867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/soict-hackathon-2024-legal-document-retrieval/Train/corpus.csv\")\n",
    "\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "print(\"Tokenizing corpus text...\")\n",
    "df[\"text\"] = df[\"text\"].progress_apply(tokenize_text)\n",
    "\n",
    "df.to_csv(\"corpus_tokenized.csv\", index=False)\n",
    "print(\"Finished corpus_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:47:48.208672Z",
     "iopub.status.busy": "2025-10-15T15:47:48.208376Z",
     "iopub.status.idle": "2025-10-15T15:48:09.538946Z",
     "shell.execute_reply": "2025-10-15T15:48:09.538170Z",
     "shell.execute_reply.started": "2025-10-15T15:47:48.208645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 86575, Val size: 9620, Test size: 10689\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "\n",
    "def create_contrastive_dataset(df):\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    for _, row in df.iterrows():\n",
    "        q = str(row[\"question\"])\n",
    "        ctx = row[\"context\"]\n",
    "        if isinstance(ctx, str) and ctx.startswith(\"[\"):\n",
    "            try:\n",
    "                lst = ast.literal_eval(ctx)\n",
    "                ctx = lst[0] if len(lst) > 0 else \"\"\n",
    "            except:\n",
    "                ctx = \"\"\n",
    "        anchors.append(q)\n",
    "        positives.append(str(ctx))\n",
    "    return Dataset.from_dict({\"anchor\": anchors, \"positive\": positives})\n",
    "\n",
    "train_df = pd.read_csv(\"/kaggle/working/train_split.csv\").dropna(subset=[\"cid\"])\n",
    "val_df = pd.read_csv(\"/kaggle/working/val_split.csv\").dropna(subset=[\"cid\"])\n",
    "test_df = pd.read_csv(\"/kaggle/working/test_split.csv\").dropna(subset=[\"cid\"])\n",
    "\n",
    "# Convert\n",
    "train_dataset = create_contrastive_dataset(train_df)\n",
    "val_dataset = create_contrastive_dataset(val_df)\n",
    "test_dataset = create_contrastive_dataset(test_df)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Pretrained Vietnamese Bi-Encoder\n",
    "model_name = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "# Loss function\n",
    "loss = losses.CachedMultipleNegativesRankingLoss(model, mini_batch_size = 1024)\n",
    "# loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T07:55:21.570591Z",
     "iopub.status.busy": "2025-10-13T07:55:21.570407Z",
     "iopub.status.idle": "2025-10-13T07:55:21.607677Z",
     "shell.execute_reply": "2025-10-13T07:55:21.607172Z",
     "shell.execute_reply.started": "2025-10-13T07:55:21.570576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sentence_transformers import SentenceTransformerTrainingArguments, SentenceTransformerTrainer\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"models/BKAI\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=300,\n",
    "    save_steps=300,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    # dataloader_num_workers=0,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "num_samples = len(train_dataset)\n",
    "num_training_steps = (num_samples // args.per_device_train_batch_size) * args.num_train_epochs\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=int(len(train_dataset)*0.1), num_training_steps=num_training_steps)\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T07:55:21.609019Z",
     "iopub.status.busy": "2025-10-13T07:55:21.608808Z",
     "iopub.status.idle": "2025-10-13T09:40:22.559424Z",
     "shell.execute_reply": "2025-10-13T09:40:22.558805Z",
     "shell.execute_reply.started": "2025-10-13T07:55:21.609003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    loss=loss,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:45:29.344722Z",
     "iopub.status.busy": "2025-10-13T15:45:29.344400Z",
     "iopub.status.idle": "2025-10-13T15:45:34.406040Z",
     "shell.execute_reply": "2025-10-13T15:45:34.405319Z",
     "shell.execute_reply.started": "2025-10-13T15:45:29.344700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"/kaggle/input/bkai-checkpoint-2100/pytorch/default/1\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-13T15:45:25.276671Z",
     "iopub.status.idle": "2025-10-13T15:45:25.276896Z",
     "shell.execute_reply": "2025-10-13T15:45:25.276794Z",
     "shell.execute_reply.started": "2025-10-13T15:45:25.276784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv(\"/kaggle/input/datause/corpus_tokenized.csv\")\n",
    "corpus_texts = corpus_df[\"text\"].tolist()\n",
    "corpus_cids = corpus_df[\"cid\"].tolist()\n",
    "corpus_embeddings = model.encode(corpus_texts, convert_to_tensor=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:03:31.356795Z",
     "iopub.status.busy": "2025-10-13T15:03:31.356095Z",
     "iopub.status.idle": "2025-10-13T15:03:35.729316Z",
     "shell.execute_reply": "2025-10-13T15:03:35.728507Z",
     "shell.execute_reply.started": "2025-10-13T15:03:31.356771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save({\n",
    "    \"embeddings\": corpus_embeddings,\n",
    "    \"cids\": corpus_cids,\n",
    "    \"texts\": corpus_texts\n",
    "}, \"/kaggle/working/corpus_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:03:53.385569Z",
     "iopub.status.busy": "2025-10-13T15:03:53.384636Z",
     "iopub.status.idle": "2025-10-13T15:03:53.393699Z",
     "shell.execute_reply": "2025-10-13T15:03:53.392867Z",
     "shell.execute_reply.started": "2025-10-13T15:03:53.385542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_with_failures(model, test_df, corpus_embeddings, corpus_cids, k_list=[1,5,10,15,20,30,40,50,100,200], device=\"cuda\"):\n",
    "    model.eval()\n",
    "    questions = test_df[\"question\"].tolist()\n",
    "    true_cids = test_df[\"cid\"].tolist()\n",
    "\n",
    "    q_emb = model.encode(questions, convert_to_tensor=True, device=device, show_progress_bar=True)\n",
    "    q_emb = F.normalize(q_emb, p=2, dim=1)\n",
    "    corpus_embeddings = F.normalize(corpus_embeddings, p=2, dim=1)\n",
    "\n",
    "    scores = torch.matmul(q_emb, corpus_embeddings.T)\n",
    "\n",
    "    recalls = {k: 0 for k in k_list}\n",
    "    mrr_total = 0.0\n",
    "    failed_cases = []\n",
    "\n",
    "    for i, cid in enumerate(true_cids):\n",
    "        topk_idx = torch.topk(scores[i], max(k_list)).indices\n",
    "        topk_cids = [corpus_cids[idx] for idx in topk_idx.cpu().numpy()]\n",
    "\n",
    "        # Recall@k\n",
    "        for k in k_list:\n",
    "            if cid in topk_cids[:k]:\n",
    "                recalls[k] += 1\n",
    "\n",
    "        # MRR\n",
    "        if cid in topk_cids:\n",
    "            rank = topk_cids.index(cid) + 1\n",
    "            mrr_total += 1.0 / rank\n",
    "        else:\n",
    "            failed_cases.append({\n",
    "                \"question\": questions[i],\n",
    "                \"true_cid\": cid,\n",
    "                \"top100_cids\": topk_cids[:100]\n",
    "            })\n",
    "\n",
    "    n = len(test_df)\n",
    "    recalls = {k: v / n for k, v in recalls.items()}\n",
    "    mrr = mrr_total / n\n",
    "\n",
    "    return recalls, mrr, failed_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:03:57.154867Z",
     "iopub.status.busy": "2025-10-13T15:03:57.154144Z",
     "iopub.status.idle": "2025-10-13T15:04:14.874843Z",
     "shell.execute_reply": "2025-10-13T15:04:14.874085Z",
     "shell.execute_reply.started": "2025-10-13T15:03:57.154836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recalls, mrr, failed_cases = evaluate_with_failures(\n",
    "    model, test_df, corpus_embeddings, corpus_cids, device=device\n",
    ")\n",
    "\n",
    "print(\"Test Metrics:\")\n",
    "for k, v in recalls.items():\n",
    "    print(f\"Recall@{k}: {v:.4f}\")\n",
    "print(f\"MRR: {mrr:.4f}\")\n",
    "\n",
    "failed_df = pd.DataFrame(failed_cases)\n",
    "failed_df.to_csv(\"failed_recall100.csv\", index=False)\n",
    "print(f\"Failed cases @100: {len(failed_df)} saved to failed_recall100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use multi-query to increase recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:45:36.149063Z",
     "iopub.status.busy": "2025-10-13T15:45:36.148686Z",
     "iopub.status.idle": "2025-10-13T15:45:48.230447Z",
     "shell.execute_reply": "2025-10-13T15:45:48.229799Z",
     "shell.execute_reply.started": "2025-10-13T15:45:36.149040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = torch.load(\"/kaggle/input/corpus-embedding/corpus_embeddings.pt\", map_location=\"cuda\")\n",
    "corpus_embeddings = data[\"embeddings\"]\n",
    "corpus_cids = data[\"cids\"]\n",
    "corpus_texts = data[\"texts\"]\n",
    "\n",
    "print(\"Loaded embeddings:\", corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T15:59:59.605318Z",
     "iopub.status.busy": "2025-10-13T15:59:59.604530Z",
     "iopub.status.idle": "2025-10-13T15:59:59.610508Z",
     "shell.execute_reply": "2025-10-13T15:59:59.609638Z",
     "shell.execute_reply.started": "2025-10-13T15:59:59.605289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import openai\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.nn import functional as F\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# cosine_threshold = 0.6\n",
    "# k_list = [1,5,10,15,20,30,40,50,100]\n",
    "\n",
    "# api_key = input(\"Enter your OpenAI API key: \")\n",
    "# openai.api_key = api_key\n",
    "\n",
    "# test_df = pd.read_csv(\"/kaggle/working/test_split.csv\").dropna(subset=[\"cid\"])\n",
    "# questions = test_df[\"question\"].tolist()\n",
    "# true_cids = test_df[\"cid\"].tolist()\n",
    "\n",
    "# q_emb = model.encode(questions, convert_to_tensor=True, device=device)\n",
    "# q_emb = F.normalize(q_emb, p=2, dim=1)\n",
    "# scores = torch.matmul(q_emb, corpus_embeddings.T)\n",
    "\n",
    "# recalls = {k: 0 for k in k_list}\n",
    "# mrr_total = 0.0\n",
    "# low_confidence_questions = []\n",
    "\n",
    "# for i, cid in enumerate(true_cids):\n",
    "#     topk_idx = torch.topk(scores[i], max(k_list)).indices\n",
    "#     topk_cids = [corpus_cids[idx] for idx in topk_idx.cpu().numpy()]\n",
    "#     max_cosine = scores[i].max().item()\n",
    "\n",
    "#     # Recall@k\n",
    "#     for k in k_list:\n",
    "#         if cid in topk_cids[:k]:\n",
    "#             recalls[k] += 1\n",
    "\n",
    "#     # MRR\n",
    "#     if cid in topk_cids:\n",
    "#         rank = topk_cids.index(cid) + 1\n",
    "#         mrr_total += 1.0 / rank\n",
    "\n",
    "#     if max_cosine < cosine_threshold:\n",
    "#         low_confidence_questions.append({\n",
    "#             \"question\": questions[i],\n",
    "#             \"true_cid\": cid,\n",
    "#             \"top100_cids\": topk_cids[:100],\n",
    "#             \"max_cosine\": max_cosine\n",
    "#         })\n",
    "\n",
    "# def generate_paraphrases(query, n=3):\n",
    "#     try:\n",
    "#         response = openai.ChatCompletion.create(\n",
    "#             model=\"gpt-4o-\",\n",
    "#             messages=[{\"role\": \"user\", \"content\": f\"Paraphrase the following question in {n} different ways: {query}\"}],\n",
    "#             temperature=0.7\n",
    "#         )\n",
    "#         texts = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "#         return [t for t in texts if t]\n",
    "#     except Exception as e:\n",
    "#         print(\"GPT error:\", e)\n",
    "#         return []\n",
    "\n",
    "# for item in low_confidence_questions:\n",
    "#     paraphrases = generate_paraphrases(item[\"question\"], n=3)\n",
    "#     item[\"paraphrases\"] = paraphrases\n",
    "\n",
    "# all_queries = questions.copy()\n",
    "# for item in low_confidence_questions:\n",
    "#     all_queries.extend(item[\"paraphrases\"])\n",
    "\n",
    "# all_q_emb = model.encode(all_queries, convert_to_tensor=True, device=device)\n",
    "# all_q_emb = F.normalize(all_q_emb, p=2, dim=1)\n",
    "# scores_updated = torch.matmul(all_q_emb, corpus_embeddings.T)\n",
    "\n",
    "# recalls_updated = {k: 0 for k in k_list}\n",
    "# mrr_total_updated = 0.0\n",
    "\n",
    "# for i, cid in enumerate(true_cids):\n",
    "#     idxs = [i]\n",
    "#     parap_count = len(low_confidence_questions[i][\"paraphrases\"]) if i < len(low_confidence_questions) else 0\n",
    "#     idxs.extend(range(len(all_queries)-parap_count, len(all_queries)))\n",
    "\n",
    "#     best_score = -1\n",
    "#     best_rank = None\n",
    "#     for idx in idxs:\n",
    "#         topk_idx = torch.topk(scores_updated[idx], max(k_list)).indices\n",
    "#         topk_cids = [corpus_cids[idx2] for idx2 in topk_idx.cpu().numpy()]\n",
    "\n",
    "#         for k in k_list:\n",
    "#             if cid in topk_cids[:k]:\n",
    "#                 recalls_updated[k] += 1\n",
    "\n",
    "#         if cid in topk_cids:\n",
    "#             rank = topk_cids.index(cid) + 1\n",
    "#             if best_score < 1.0/rank:\n",
    "#                 best_score = 1.0/rank\n",
    "#                 best_rank = rank\n",
    "\n",
    "#     if best_rank is not None:\n",
    "#         mrr_total_updated += best_score\n",
    "\n",
    "# n = len(true_cids)\n",
    "# recalls_updated = {k: v / n for k, v in recalls_updated.items()}\n",
    "# mrr_updated = mrr_total_updated / n\n",
    "\n",
    "# print(\"Updated Test Metrics after multi-query generation:\")\n",
    "# for k, v in recalls_updated.items():\n",
    "#     print(f\"Recall@{k}: {v:.4f}\")\n",
    "# print(f\"MRR: {mrr_updated:.4f}\")\n",
    "\n",
    "# # pd.DataFrame(low_confidence_questions).to_csv(\"low_confidence_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:48:09.543241Z",
     "iopub.status.busy": "2025-10-15T15:48:09.542993Z",
     "iopub.status.idle": "2025-10-15T15:48:30.241281Z",
     "shell.execute_reply": "2025-10-15T15:48:30.240395Z",
     "shell.execute_reply.started": "2025-10-15T15:48:09.543221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 15:48:15.641091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760543295.825154     132 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760543295.875089     132 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"/kaggle/input/bkai-checkpoint-2100/pytorch/default/1\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:48:30.242772Z",
     "iopub.status.busy": "2025-10-15T15:48:30.242474Z",
     "iopub.status.idle": "2025-10-15T15:48:33.176521Z",
     "shell.execute_reply": "2025-10-15T15:48:33.175611Z",
     "shell.execute_reply.started": "2025-10-15T15:48:30.242738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>cid</th>\n",
       "      <th>qid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Người học ngành quản_lý khai_thác công_trình t...</td>\n",
       "      <td>['Khả_năng học_tập , nâng cao_trình_độ \\n - Kh...</td>\n",
       "      <td>62492</td>\n",
       "      <td>161615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nội_dung lồng_ghép vấn_đề bình_đẳng giới trong...</td>\n",
       "      <td>['Nội_dung lồng_ghép vấn_đề bình_đẳng giới tro...</td>\n",
       "      <td>151154</td>\n",
       "      <td>80037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sản_phẩm phần_mềm có được hưởng ưu_đãi về thời...</td>\n",
       "      <td>['\" Điều 20 . Ưu_đãi về thời_gian miễn thuế , ...</td>\n",
       "      <td>75071</td>\n",
       "      <td>124074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Điều_kiện để giáo_viên trong cơ_sở giáo_dục mầ...</td>\n",
       "      <td>['Điều_kiện được hưởng \\n Cán_bộ quản_lý , giá...</td>\n",
       "      <td>225897</td>\n",
       "      <td>146841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nguyên_tắc áp_dụng phụ_cấp ưu_đãi nghề y_tế th...</td>\n",
       "      <td>['Nguyên_tắc áp_dụng \\n 1 . Trường_hợp công_ch...</td>\n",
       "      <td>68365</td>\n",
       "      <td>6176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Người học ngành quản_lý khai_thác công_trình t...   \n",
       "1  Nội_dung lồng_ghép vấn_đề bình_đẳng giới trong...   \n",
       "2  Sản_phẩm phần_mềm có được hưởng ưu_đãi về thời...   \n",
       "3  Điều_kiện để giáo_viên trong cơ_sở giáo_dục mầ...   \n",
       "4  Nguyên_tắc áp_dụng phụ_cấp ưu_đãi nghề y_tế th...   \n",
       "\n",
       "                                             context     cid     qid  \n",
       "0  ['Khả_năng học_tập , nâng cao_trình_độ \\n - Kh...   62492  161615  \n",
       "1  ['Nội_dung lồng_ghép vấn_đề bình_đẳng giới tro...  151154   80037  \n",
       "2  ['\" Điều 20 . Ưu_đãi về thời_gian miễn thuế , ...   75071  124074  \n",
       "3  ['Điều_kiện được hưởng \\n Cán_bộ quản_lý , giá...  225897  146841  \n",
       "4  ['Nguyên_tắc áp_dụng \\n 1 . Trường_hợp công_ch...   68365    6176  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"/kaggle/input/datause/train_tokenized.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:48:33.177713Z",
     "iopub.status.busy": "2025-10-15T15:48:33.177426Z",
     "iopub.status.idle": "2025-10-15T15:48:42.295208Z",
     "shell.execute_reply": "2025-10-15T15:48:42.294457Z",
     "shell.execute_reply.started": "2025-10-15T15:48:33.177693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings: torch.Size([261597, 768])\n",
      "Number of CIDs: 261597\n",
      "Number of texts: 261597\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def load_corpus_embeddings(pt_path, device=\"cuda\"):\n",
    "    data = torch.load(pt_path, map_location=device)\n",
    "    \n",
    "    corpus_embeddings = data[\"embeddings\"]\n",
    "    corpus_cids = data[\"cids\"]\n",
    "    corpus_texts = data[\"texts\"]\n",
    "    \n",
    "    print(\"Loaded embeddings:\", corpus_embeddings.shape)\n",
    "    print(\"Number of CIDs:\", len(corpus_cids))\n",
    "    print(\"Number of texts:\", len(corpus_texts))\n",
    "    \n",
    "    return corpus_embeddings, corpus_cids, corpus_texts\n",
    "\n",
    "embeddings, cids, texts = load_corpus_embeddings(\"/kaggle/input/corpus-embedding/corpus_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:48:42.296951Z",
     "iopub.status.busy": "2025-10-15T15:48:42.296684Z",
     "iopub.status.idle": "2025-10-15T15:48:53.132729Z",
     "shell.execute_reply": "2025-10-15T15:48:53.131994Z",
     "shell.execute_reply.started": "2025-10-15T15:48:42.296905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thông_tư này hướng_dẫn tuần_tra , canh_gác bảo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 . Hàng năm trước mùa mưa , lũ , Ủy_ban nhân_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tiêu_chuẩn của các thành_viên thuộc lực_lượng ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nhiệm_vụ của lực_lượng tuần_tra , canh_gác đê ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phù_hiệu của lực_lượng tuần_tra , canh_gác đê ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cid\n",
       "0  Thông_tư này hướng_dẫn tuần_tra , canh_gác bảo...    0\n",
       "1  1 . Hàng năm trước mùa mưa , lũ , Ủy_ban nhân_...    1\n",
       "2  Tiêu_chuẩn của các thành_viên thuộc lực_lượng ...    2\n",
       "3  Nhiệm_vụ của lực_lượng tuần_tra , canh_gác đê ...    3\n",
       "4  Phù_hiệu của lực_lượng tuần_tra , canh_gác đê ...    4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"/kaggle/input/datause/corpus_tokenized.csv\")\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T15:48:53.133763Z",
     "iopub.status.busy": "2025-10-15T15:48:53.133496Z",
     "iopub.status.idle": "2025-10-15T16:00:01.678159Z",
     "shell.execute_reply": "2025-10-15T16:00:01.677355Z",
     "shell.execute_reply.started": "2025-10-15T15:48:53.133737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3341/3341 [11:08<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import util\n",
    "\n",
    "train['negative_cid'] = None\n",
    "train['negative_context'] = None\n",
    "batch_size = 32\n",
    "\n",
    "for start_idx in tqdm(range(0, len(train), batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, len(train))\n",
    "    batch = train.iloc[start_idx:end_idx]\n",
    "\n",
    "    questions = batch['question'].tolist()\n",
    "    query_embeddings = model.encode(questions, show_progress_bar=False)\n",
    "\n",
    "    batch_hits = util.semantic_search(query_embeddings, embeddings, top_k=50)\n",
    "\n",
    "    for i, (idx, row) in enumerate(batch.iterrows()):\n",
    "        if isinstance(row['cid'], (list, tuple)):\n",
    "            positive_cids = set(row['cid'])\n",
    "        else:\n",
    "            positive_cids = {row['cid']}\n",
    "\n",
    "        candidates = []\n",
    "        for hit in batch_hits[i]:\n",
    "            cid = cids[hit['corpus_id']]\n",
    "            if cid not in positive_cids:\n",
    "                candidates.append(cid)\n",
    "\n",
    "        if len(candidates) > 0:\n",
    "            num_samples = min(10, len(candidates))\n",
    "            neg_samples = random.sample(candidates, num_samples)\n",
    "            train.at[idx, 'negative_cid'] = neg_samples\n",
    "            train.at[idx, 'negative_context'] = [corpus.loc[corpus['cid'] == cid, 'text'].values[0] for cid in neg_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T16:00:01.679163Z",
     "iopub.status.busy": "2025-10-15T16:00:01.678951Z",
     "iopub.status.idle": "2025-10-15T16:01:04.115374Z",
     "shell.execute_reply": "2025-10-15T16:01:04.114462Z",
     "shell.execute_reply.started": "2025-10-15T16:00:01.679144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"/kaggle/working/train_cross_encoder.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T16:01:04.116689Z",
     "iopub.status.busy": "2025-10-15T16:01:04.116388Z",
     "iopub.status.idle": "2025-10-15T16:01:37.863141Z",
     "shell.execute_reply": "2025-10-15T16:01:37.862274Z",
     "shell.execute_reply.started": "2025-10-15T16:01:04.116662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>cid</th>\n",
       "      <th>qid</th>\n",
       "      <th>negative_cid</th>\n",
       "      <th>negative_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Người học ngành quản_lý khai_thác công_trình t...</td>\n",
       "      <td>['Khả_năng học_tập , nâng cao_trình_độ \\n - Kh...</td>\n",
       "      <td>62492</td>\n",
       "      <td>161615</td>\n",
       "      <td>[505847, 603422, 592734, 503390, 476410, 92590...</td>\n",
       "      <td>['Điều 1 . Ban_hành kèm theo Thông_tư này quy_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nội_dung lồng_ghép vấn_đề bình_đẳng giới trong...</td>\n",
       "      <td>['Nội_dung lồng_ghép vấn_đề bình_đẳng giới tro...</td>\n",
       "      <td>151154</td>\n",
       "      <td>80037</td>\n",
       "      <td>[461115, 133919, 622165, 15113, 591096, 26109,...</td>\n",
       "      <td>['Khoản 3.1 - Nâng cao nhận_thức về công_tác p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sản_phẩm phần_mềm có được hưởng ưu_đãi về thời...</td>\n",
       "      <td>['\" Điều 20 . Ưu_đãi về thời_gian miễn thuế , ...</td>\n",
       "      <td>75071</td>\n",
       "      <td>124074</td>\n",
       "      <td>[233023, 631699, 629610, 561309, 498073, 73603...</td>\n",
       "      <td>['Ưu_đãi miễn , giảm thuế thu_nhập doanh_nghiệ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Điều_kiện để giáo_viên trong cơ_sở giáo_dục mầ...</td>\n",
       "      <td>['Điều_kiện được hưởng \\n Cán_bộ quản_lý , giá...</td>\n",
       "      <td>225897</td>\n",
       "      <td>146841</td>\n",
       "      <td>[56406, 16908, 637221, 603297, 578013, 14311, ...</td>\n",
       "      <td>['Quyết_định này Quy_định về một_số chính_sách...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nguyên_tắc áp_dụng phụ_cấp ưu_đãi nghề y_tế th...</td>\n",
       "      <td>['Nguyên_tắc áp_dụng \\n 1 . Trường_hợp công_ch...</td>\n",
       "      <td>68365</td>\n",
       "      <td>6176</td>\n",
       "      <td>[54637, 172835, 223959, 63970, 44729, 159194, ...</td>\n",
       "      <td>['\" Điều 3 . Nguyên_tắc áp_dụng \\n 1 . Cán_bộ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Người học ngành quản_lý khai_thác công_trình t...   \n",
       "1  Nội_dung lồng_ghép vấn_đề bình_đẳng giới trong...   \n",
       "2  Sản_phẩm phần_mềm có được hưởng ưu_đãi về thời...   \n",
       "3  Điều_kiện để giáo_viên trong cơ_sở giáo_dục mầ...   \n",
       "4  Nguyên_tắc áp_dụng phụ_cấp ưu_đãi nghề y_tế th...   \n",
       "\n",
       "                                             context     cid     qid  \\\n",
       "0  ['Khả_năng học_tập , nâng cao_trình_độ \\n - Kh...   62492  161615   \n",
       "1  ['Nội_dung lồng_ghép vấn_đề bình_đẳng giới tro...  151154   80037   \n",
       "2  ['\" Điều 20 . Ưu_đãi về thời_gian miễn thuế , ...   75071  124074   \n",
       "3  ['Điều_kiện được hưởng \\n Cán_bộ quản_lý , giá...  225897  146841   \n",
       "4  ['Nguyên_tắc áp_dụng \\n 1 . Trường_hợp công_ch...   68365    6176   \n",
       "\n",
       "                                        negative_cid  \\\n",
       "0  [505847, 603422, 592734, 503390, 476410, 92590...   \n",
       "1  [461115, 133919, 622165, 15113, 591096, 26109,...   \n",
       "2  [233023, 631699, 629610, 561309, 498073, 73603...   \n",
       "3  [56406, 16908, 637221, 603297, 578013, 14311, ...   \n",
       "4  [54637, 172835, 223959, 63970, 44729, 159194, ...   \n",
       "\n",
       "                                    negative_context  \n",
       "0  ['Điều 1 . Ban_hành kèm theo Thông_tư này quy_...  \n",
       "1  ['Khoản 3.1 - Nâng cao nhận_thức về công_tác p...  \n",
       "2  ['Ưu_đãi miễn , giảm thuế thu_nhập doanh_nghiệ...  \n",
       "3  ['Quyết_định này Quy_định về một_số chính_sách...  \n",
       "4  ['\" Điều 3 . Nguyên_tắc áp_dụng \\n 1 . Cán_bộ ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/working/train_cross_encoder.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T16:01:37.864322Z",
     "iopub.status.busy": "2025-10-15T16:01:37.864078Z",
     "iopub.status.idle": "2025-10-15T16:04:32.148185Z",
     "shell.execute_reply": "2025-10-15T16:04:32.147296Z",
     "shell.execute_reply.started": "2025-10-15T16:01:37.864303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>cid</th>\n",
       "      <th>qid</th>\n",
       "      <th>negative_cid</th>\n",
       "      <th>negative_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Người học ngành quản_lý khai_thác công_trình t...</td>\n",
       "      <td>[Khả_năng học_tập , nâng cao_trình_độ \\n - Khố...</td>\n",
       "      <td>[62492]</td>\n",
       "      <td>161615</td>\n",
       "      <td>[505847, 603422, 592734, 503390, 476410, 92590...</td>\n",
       "      <td>[Điều 1 . Ban_hành kèm theo Thông_tư này quy_đ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nội_dung lồng_ghép vấn_đề bình_đẳng giới trong...</td>\n",
       "      <td>[Nội_dung lồng_ghép vấn_đề bình_đẳng giới tron...</td>\n",
       "      <td>[151154]</td>\n",
       "      <td>80037</td>\n",
       "      <td>[461115, 133919, 622165, 15113, 591096, 26109,...</td>\n",
       "      <td>[Khoản 3.1 - Nâng cao nhận_thức về công_tác ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sản_phẩm phần_mềm có được hưởng ưu_đãi về thời...</td>\n",
       "      <td>[\" Điều 20 . Ưu_đãi về thời_gian miễn thuế , g...</td>\n",
       "      <td>[75071]</td>\n",
       "      <td>124074</td>\n",
       "      <td>[233023, 631699, 629610, 561309, 498073, 73603...</td>\n",
       "      <td>[Ưu_đãi miễn , giảm thuế thu_nhập doanh_nghiệp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Điều_kiện để giáo_viên trong cơ_sở giáo_dục mầ...</td>\n",
       "      <td>[Điều_kiện được hưởng \\n Cán_bộ quản_lý , giáo...</td>\n",
       "      <td>[225897]</td>\n",
       "      <td>146841</td>\n",
       "      <td>[56406, 16908, 637221, 603297, 578013, 14311, ...</td>\n",
       "      <td>[Quyết_định này Quy_định về một_số chính_sách ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nguyên_tắc áp_dụng phụ_cấp ưu_đãi nghề y_tế th...</td>\n",
       "      <td>[Nguyên_tắc áp_dụng \\n 1 . Trường_hợp công_chứ...</td>\n",
       "      <td>[68365]</td>\n",
       "      <td>6176</td>\n",
       "      <td>[54637, 172835, 223959, 63970, 44729, 159194, ...</td>\n",
       "      <td>[\" Điều 3 . Nguyên_tắc áp_dụng \\n 1 . Cán_bộ ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Người học ngành quản_lý khai_thác công_trình t...   \n",
       "1  Nội_dung lồng_ghép vấn_đề bình_đẳng giới trong...   \n",
       "2  Sản_phẩm phần_mềm có được hưởng ưu_đãi về thời...   \n",
       "3  Điều_kiện để giáo_viên trong cơ_sở giáo_dục mầ...   \n",
       "4  Nguyên_tắc áp_dụng phụ_cấp ưu_đãi nghề y_tế th...   \n",
       "\n",
       "                                             context       cid     qid  \\\n",
       "0  [Khả_năng học_tập , nâng cao_trình_độ \\n - Khố...   [62492]  161615   \n",
       "1  [Nội_dung lồng_ghép vấn_đề bình_đẳng giới tron...  [151154]   80037   \n",
       "2  [\" Điều 20 . Ưu_đãi về thời_gian miễn thuế , g...   [75071]  124074   \n",
       "3  [Điều_kiện được hưởng \\n Cán_bộ quản_lý , giáo...  [225897]  146841   \n",
       "4  [Nguyên_tắc áp_dụng \\n 1 . Trường_hợp công_chứ...   [68365]    6176   \n",
       "\n",
       "                                        negative_cid  \\\n",
       "0  [505847, 603422, 592734, 503390, 476410, 92590...   \n",
       "1  [461115, 133919, 622165, 15113, 591096, 26109,...   \n",
       "2  [233023, 631699, 629610, 561309, 498073, 73603...   \n",
       "3  [56406, 16908, 637221, 603297, 578013, 14311, ...   \n",
       "4  [54637, 172835, 223959, 63970, 44729, 159194, ...   \n",
       "\n",
       "                                    negative_context  \n",
       "0  [Điều 1 . Ban_hành kèm theo Thông_tư này quy_đ...  \n",
       "1  [Khoản 3.1 - Nâng cao nhận_thức về công_tác ph...  \n",
       "2  [Ưu_đãi miễn , giảm thuế thu_nhập doanh_nghiệp...  \n",
       "3  [Quyết_định này Quy_định về một_số chính_sách ...  \n",
       "4  [\" Điều 3 . Nguyên_tắc áp_dụng \\n 1 . Cán_bộ ,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "columns_to_convert = [\"context\", \"cid\", \"negative_cid\", \"negative_context\"]\n",
    "for col in columns_to_convert:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "df['cid'] = df['cid'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "df['negative_cid'] = df['negative_cid'].apply(lambda x: x if isinstance(x, list) else [x])\n",
    "\n",
    "df.to_csv(\"/kaggle/working/train_cross_encoder.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:17:22.266664Z",
     "iopub.status.busy": "2025-10-15T17:17:22.266371Z",
     "iopub.status.idle": "2025-10-15T17:18:11.440211Z",
     "shell.execute_reply": "2025-10-15T17:18:11.439409Z",
     "shell.execute_reply.started": "2025-10-15T17:17:22.266640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 96195, Eval size: 10689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/train-cross-encoder/train_cross_encoder.csv\")\n",
    "\n",
    "train_data, eval_data = train_test_split(df, test_size=0.1, random_state=42)\n",
    "print(f\"Train size: {len(train_data)}, Eval size: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-14T07:58:15.684Z",
     "iopub.execute_input": "2025-10-14T07:09:00.976742Z",
     "iopub.status.busy": "2025-10-14T07:09:00.976525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers.readers import InputExample\n",
    "# from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "\n",
    "# #multi loss\n",
    "# def create_contrastive_dataset(filtered_data):\n",
    "#     train_samples = []\n",
    "\n",
    "#     for index, row in filtered_data.iterrows():\n",
    "#         query = row['question']\n",
    "#         i = 0\n",
    "\n",
    "#         for doc_positive in row['context']:\n",
    "#             train_samples.append(InputExample(\n",
    "#                     texts=[query, doc_positive],\n",
    "#                     label=1  # positive pair\n",
    "#                 ))\n",
    "\n",
    "#         for doc_negative in row['negative_context']:\n",
    "#             if i < 3: \n",
    "#                 train_samples.append(InputExample(\n",
    "#                     texts=[query, doc_negative],\n",
    "#                     label=0 # negative pair\n",
    "#                 ))\n",
    "#                 i+=1\n",
    "\n",
    "#     return train_samples\n",
    "\n",
    "# train_dataset = create_contrastive_dataset(train_data)\n",
    "# print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "# validation_dataset = create_contrastive_dataset(eval_data)\n",
    "# print(f\"Val dataset size: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:18:11.441868Z",
     "iopub.status.busy": "2025-10-15T17:18:11.441664Z",
     "iopub.status.idle": "2025-10-15T17:18:16.925012Z",
     "shell.execute_reply": "2025-10-15T17:18:16.924240Z",
     "shell.execute_reply.started": "2025-10-15T17:18:11.441852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 192390\n",
      "Val dataset size: 21378\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "\n",
    "def create_contrastive_dataset_v1(filtered_data, max_negatives_per_positive=2):\n",
    "    train_samples = []\n",
    "    for index, row in filtered_data.iterrows():\n",
    "        query = row['question']\n",
    "        \n",
    "        positive_contexts = row['context'] if isinstance(row['context'], list) else [row['context']]\n",
    "        negative_contexts = row['negative_context'] if isinstance(row['negative_context'], list) else [row['negative_context']]\n",
    "        \n",
    "        for doc_positive in positive_contexts:\n",
    "            train_samples.append(InputExample(\n",
    "                texts=[query, doc_positive],\n",
    "                label=1\n",
    "            ))\n",
    "            \n",
    "            for doc_negative in negative_contexts[:max_negatives_per_positive]:\n",
    "                train_samples.append(InputExample(\n",
    "                    texts=[query, doc_negative],\n",
    "                    label=0\n",
    "                ))\n",
    "    \n",
    "    return train_samples\n",
    "\n",
    "train_dataset = create_contrastive_dataset_v1(train_data)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "validation_dataset = create_contrastive_dataset_v1(eval_data)\n",
    "print(f\"Val dataset size: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:18:16.926051Z",
     "iopub.status.busy": "2025-10-15T17:18:16.925783Z",
     "iopub.status.idle": "2025-10-15T17:18:22.626486Z",
     "shell.execute_reply": "2025-10-15T17:18:22.625945Z",
     "shell.execute_reply.started": "2025-10-15T17:18:16.926028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2877d1d8cd4940916e0c82d7c22d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/814 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e5ee1c8d124d2cb5adb3833d173d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65077b3a8b2b4e8a82ef9bbecfca375c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db946c1bd4540f29889de6ef09b74d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40467087e9ab42a1a970ad22de58b8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3a8f7296fd487a8bd7dd9eb19528c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7134a75276948579fbefd6d4bd80b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer,CrossEncoder, SentenceTransformerTrainer, losses, SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CrossEncoder('itdainb/PhoRanker', max_length=256, num_labels=1)\n",
    "model.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:18:22.628388Z",
     "iopub.status.busy": "2025-10-15T17:18:22.628187Z",
     "iopub.status.idle": "2025-10-15T17:18:22.635171Z",
     "shell.execute_reply": "2025-10-15T17:18:22.634472Z",
     "shell.execute_reply.started": "2025-10-15T17:18:22.628373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "evaluator = CEBinaryClassificationEvaluator.from_input_examples(validation_dataset, name=\"Quora-dev\")\n",
    "#loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0])).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:18:22.636441Z",
     "iopub.status.busy": "2025-10-15T17:18:22.636042Z",
     "iopub.status.idle": "2025-10-15T17:18:23.149285Z",
     "shell.execute_reply": "2025-10-15T17:18:23.148452Z",
     "shell.execute_reply.started": "2025-10-15T17:18:22.636415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:18:23.150338Z",
     "iopub.status.busy": "2025-10-15T17:18:23.150062Z",
     "iopub.status.idle": "2025-10-15T17:18:23.163156Z",
     "shell.execute_reply": "2025-10-15T17:18:23.162624Z",
     "shell.execute_reply.started": "2025-10-15T17:18:23.150309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# early_stop = EarlyStoppingCallback(\n",
    "#     monitor='val_loss',\n",
    "#     patience=3,\n",
    "#     min_delta=0.0001\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T17:18:23.164206Z",
     "iopub.status.busy": "2025-10-15T17:18:23.163971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4001' max='96196' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4001/96196 20:34 < 7:54:25, 3.24 it/s, Epoch 0.08/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Quora-dev Accuracy</th>\n",
       "      <th>Quora-dev Accuracy Threshold</th>\n",
       "      <th>Quora-dev F1</th>\n",
       "      <th>Quora-dev F1 Threshold</th>\n",
       "      <th>Quora-dev Precision</th>\n",
       "      <th>Quora-dev Recall</th>\n",
       "      <th>Quora-dev Average Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.946721</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.946271</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.954329</td>\n",
       "      <td>0.938348</td>\n",
       "      <td>0.990273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.942277</td>\n",
       "      <td>0.981106</td>\n",
       "      <td>0.941962</td>\n",
       "      <td>0.981106</td>\n",
       "      <td>0.947129</td>\n",
       "      <td>0.936851</td>\n",
       "      <td>0.987568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.946393</td>\n",
       "      <td>0.974146</td>\n",
       "      <td>0.946263</td>\n",
       "      <td>0.973831</td>\n",
       "      <td>0.948576</td>\n",
       "      <td>0.943961</td>\n",
       "      <td>0.991472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31e8c651dc94040ba0b6869d5fc1805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b460b945e8c3457b9f266881d23b5d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d236fa05464de0b287a500be868fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57684478ecf94c4ab1cc890b867cbc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=2,\n",
    "    loss_fct = loss_fn,\n",
    "    warmup_steps=int(len(train_dataset) * 0.1),\n",
    "    optimizer_params={'lr': 3e-4, 'weight_decay': 0.01},\n",
    "    evaluation_steps=1000,\n",
    "    save_best_model=True,\n",
    "    output_path='models/PhoRanker_3_rand_2',\n",
    "    show_progress_bar=True,\n",
    "    callback=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "openai.api_key = \"***\"\n",
    "\n",
    "URL = \"https://thuvienphapluat.vn/van-ban/Giao-duc/Thong-tu-21-2025-TT-BGDDT-che-do-tra-tien-luong-day-them-gio-nha-giao-trong-cac-co-so-giao-duc-cong-lap-673797.aspx\"\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "content_div = soup.find(\"div\", {\"id\": \"divNoiDung\"})\n",
    "text = content_div.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "def chunk_text(text, max_words=700):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), max_words):\n",
    "        yield \" \".join(words[i:i+max_words])\n",
    "\n",
    "chunks = list(chunk_text(text))\n",
    "\n",
    "def generate_qa_block(context, cid_start, qid_start):\n",
    "    prompt = f\"\"\"\n",
    "Tạo khoảng 5 cặp dữ liệu dạng JSON với cấu trúc:\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"...\",\n",
    "    \"context\": \"...\",\n",
    "    \"cid\": number,\n",
    "    \"qid\": number\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Trong đó:\n",
    "- question là câu hỏi ngắn, rõ, liên quan nội dung bên dưới\n",
    "- context là phần trích ngắn từ đoạn văn, chứa đủ thông tin trả lời\n",
    "- cid và qid là số duy nhất (cid cho context, qid cho từng câu hỏi)\n",
    "Đây là đoạn văn:\n",
    "{context}\n",
    "\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    try:\n",
    "        data = json.loads(response.choices[0].message.content)\n",
    "    except:\n",
    "        data = []\n",
    "    return data\n",
    "\n",
    "dataset = []\n",
    "cid_counter = 60000\n",
    "qid_counter = 160000\n",
    "\n",
    "for chunk in chunks[:200]:\n",
    "    cid_counter += 1\n",
    "    qid_counter += 1\n",
    "    data = generate_qa_block(chunk, cid_counter, qid_counter)\n",
    "    for d in data:\n",
    "        d[\"cid\"] = cid_counter\n",
    "        d[\"qid\"] = qid_counter\n",
    "        qid_counter += 1\n",
    "        dataset.append(d)\n",
    "    time.sleep(1)\n",
    "\n",
    "with open(\"qa_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dataset:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"{len(dataset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final output MRR@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.7682\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "data = torch.load(\"/kaggle/input/corpus-embedding/corpus_embeddings.pt\", map_location=\"cuda\")\n",
    "corpus_embeddings = data[\"embeddings\"]\n",
    "corpus_cids = data[\"cids\"]\n",
    "corpus_texts = data[\"texts\"]\n",
    "print(\"Loaded embeddings:\", corpus_embeddings.shape)\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/working/test_split.csv\")\n",
    "test['cid'] = test['cid'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [x])\n",
    "\n",
    "# Bi-encoder\n",
    "bi_encoder = SentenceTransformer(\"/kaggle/input/bkai-checkpoint-2100/pytorch/default/1\")\n",
    "\n",
    "# Cross-encoder\n",
    "cross_encoder = SentenceTransformer(\"/kaggle/working/models/PhoRanker_3_rand_2\")\n",
    "\n",
    "questions = test['question'].tolist()\n",
    "question_embeddings = bi_encoder.encode(questions, convert_to_tensor=True, device='cuda')\n",
    "\n",
    "top_k = 100\n",
    "hits = util.semantic_search(question_embeddings, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "reranked_hits = []\n",
    "for i, row in enumerate(test.itertuples()):\n",
    "    q = row.question\n",
    "    top_candidates = hits[i][:10]\n",
    "\n",
    "    cross_inputs = []\n",
    "    cids_list = []\n",
    "    for hit in top_candidates:\n",
    "        cid = corpus_cids[hit['corpus_id']]\n",
    "        context = corpus_texts[hit['corpus_id']]\n",
    "        cross_inputs.append([q, context])\n",
    "        cids_list.append(cid)\n",
    "\n",
    "    scores = cross_encoder.predict(cross_inputs, convert_to_tensor=True).cpu().numpy()\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    sorted_cids = [cids_list[idx] for idx in sorted_indices]\n",
    "    reranked_hits.append(sorted_cids)\n",
    "\n",
    "mrr_total = 0.0\n",
    "for i, row in enumerate(test.itertuples()):\n",
    "    pos_cids = row.cid if isinstance(row.cid, list) else [row.cid]\n",
    "    retrieved = reranked_hits[i]\n",
    "\n",
    "    rank = 0\n",
    "    for idx, cid in enumerate(retrieved, start=1):\n",
    "        if cid in pos_cids:\n",
    "            rank = idx\n",
    "            break\n",
    "    if rank > 0:\n",
    "        mrr_total += 1.0 / rank\n",
    "\n",
    "mrr_score = mrr_total / len(test)\n",
    "print(\"MRR:\", mrr_score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8353895,
     "sourceId": 13182345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8465232,
     "sourceId": 13348254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8482787,
     "sourceId": 13371165,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8500844,
     "sourceId": 13396052,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 471517,
     "modelInstanceId": 455463,
     "sourceId": 606980,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "userk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
